

## OVERVIEW

##### As I have add the host (srandazzo21 and mrbhbs) as collaborators to my private kernel,  maybe the simplest way is to run the following notebooks directly on the kaggle platform .

This approach consists of 3 single modes:

- 1D-CNN
- TabNet
- DNN

and the final submission is generated by weighted average of single models outputs.


## ARCHIVE CONTENTS

##### The following links are the codes I used to generate 3 single models and the final submission. It can run directly after copying. 

- training: notebooks for 3 single models training and model files generating.

  - 1D-CNN: https://www.kaggle.com/baosenguo/ble2-dro1-3-ns1-4-tr2
  - TabNet: https://www.kaggle.com/baosenguo/fork-of-ble-w1-6-2-kd-tab1-wgt1-pa1-4
  - DNN: https://www.kaggle.com/baosenguo/ble2-nntrans-tr

- inference & blending: https://www.kaggle.com/baosenguo/ble-f1


#### Note

Each single model notebook contains complete processes, including data and package loading,  preprocessing, feature engineering, model training and saving, etc. The 3 single models are slightly different in the above steps, which is to increase the diversity for final blending.

After running each single model script, all trained models will be saved in kernel output. Add these outputs to the inference notebook, and you can use them for inference and submission ( If you use the provided kaggle notebook links, these models have been added to the inference notebook, and you can run the final inference directly without editing ) .

For the inference notebook, it preprocesses all data according to the single model data processing mode before using each of them to predict the test set, so as to ensure that each model can be used correctly when replacing the private test set.

Finally, the predictions of 3 single models are weighted average as the final submission. 

## TRAINING
##### In this way, all the training and inference steps can be done within notebooks on kaggle platform.

1. Copy and run 3 single models notebook ( 1D-CNN, TabNet, DNN ). Choose accelerator as GPU.  After that, all trained models will be saved in kernel output.

    By using Tesla P100 provided by the kaggle platform, the training time for the 3 models are: 1D-CNN 1.90 hours, TabNet 2.65 hours, DNN 2.08 hours.

2. Copy and run the inference notebook. Since three single models have been added to this notebook as kernel output, they can be run directly.







