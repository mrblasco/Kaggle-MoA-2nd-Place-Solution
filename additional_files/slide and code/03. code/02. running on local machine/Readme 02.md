

## OVERVIEW

##### As I have add the host (srandazzo21 and mrbhbs) as collaborators to my private kernel,  maybe the simplest way is to run the following notebooks directly on the kaggle platform .

##### You can use these codes if you run the notebook locally or upload the modified notebook to the kaggle platform.

This approach consists of 3 single modes:

- 1D-CNN
- TabNet
- DNN

and the final submission is generated by weighted average of single models outputs.

## SOFTWARE

python 3.7.6

numpy 1.18.5

pandas 1.1.3

torch 1.6.0

pytorch_tabnet 2.0.0

sklearn 0.23.2


## ARCHIVE CONTENTS

##### The following notebooks are the codes I used to generate 3 single models and the final submission. 

- .\input\lish-moa
  - The folder is required by all four scripts (three single models and one reference, input_dir = '../input/lish-moa/')
  - If you are running data locally, place these files in this folder.
  -  If you upload your code and run on the kaggle platform, you can add the MOA dataset directly and don`t have to creat this fold. 
  -  These two ways will get the same result.

- training: notebooks for 3 single models training and model files generating.

  - 1D-CNN: https://www.kaggle.com/baosenguo/ble2-dro1-3-ns1-4-tr2
  - TabNet: https://www.kaggle.com/baosenguo/fork-of-ble-w1-6-2-kd-tab1-wgt1-pa1-4
  - DNN: https://www.kaggle.com/baosenguo/ble2-nntrans-tr

  - After the three scripts run, the model file will be generated in the same folder as code. Or you can specify the location of the model generation by adding a path before the model name of the script. 

- inference & blending: https://www.kaggle.com/baosenguo/ble-f1

  - In this script, in addition to input_dir, you also need to specify the model file path of three single models. mod_path1，mod_path2，mod_path3 corresponds to 1d-cnn, tabnet and DNN respectively.
  - If you run the file locally without modifying the save path of the model file, mod_path1，mod_path2，mod_path3 should all be "./"
  - If you run locally and change the model save path, you should change it to this path.
  - If you are running the single model code on the kaggle platform, you can add the kernel output of the single model to the information script as data, and then copy the added data

#### Note

Each single model notebook contains complete processes, including data and package loading,  preprocessing, feature engineering, model training and saving, etc. The 3 single models are slightly different in the above steps, which is to increase the diversity for final blending.

After running each single model script, all trained models will be saved in kernel output (if run on kaggle) or in current folder (if run on local machine).  Add these model files to the inference notebook (if run on kaggle) or copy the folder to the mod_path (if run on local machine), and you can use them for inference and submission.

For the inference notebook, it preprocesses all data according to the single model data processing mode before using each of them to predict the test set, so as to ensure that each model can be used correctly when replacing the private test set.

Finally, the predictions of 3 single models are weighted average as the final submission. 

## TRAINING
##### In this way, all the training and inference steps can be done within notebooks on kaggle platform.

1. Copy all input files to .\input\lish-moa.

2. Run 3 single models notebook ( 1D-CNN, TabNet, DNN ). Choose accelerator as GPU.  After that, all trained models will be saved in kernel output (if run on kaggle) or in current folder (if run on local machine).

    By using Tesla P100 provided by the kaggle platform, the training time for the 3 models are: 1D-CNN 1.90 hours, TabNet 2.65 hours, DNN 2.08 hours.

3. Copy your model output path to mod_path1,mod_path2 and mod_path3. Run inference notebook, predictions of 3 single models are generated and be weighted average as the final submission. 







